\section{Применяемые методы трекинга}

\subsection{Формальная постановка задачи и используемые обозначения}

Алгоритм трекинга принимает на вход последовательность кадров "---
RGB-изображений
$
    \Img_i \colon \ImgDom \to \Real^3
$,
где $\ImgDom \subset \Real^2$ "--- область определения изображения.
Таким образом, $\Img_i(\vect{u})$ "--- значение цвета, которое принимает
изображение $\Img_i$ в точке $\vect{u} \in \ImgDom$.
В тех случаях, когда номер кадра неважен, он будет опускаться.

Внутренние параметры камеры, на которую сняты входные кадры, задаются матрицей
\begin{equation}\label{eqn:camintr}
    \CamIntr = \begin{bmatrix}
        f_x & 0   & c_x \\
        0   & f_y & c_y \\
        0   & 0   & 1
    \end{bmatrix}
    \text{.}
\end{equation}
Они одинаковы для всех кадров видео.

Форма отслеживаемого объекта задается 3D-моделью $\Mesh$.
Определим ее как пару $(\MeshV, \MeshF)$, где
$\MeshV \subset \Real^3$~--- конечное множество вершин модели, а
$\MeshF$~--- множество троек вершин, задающих грани модели.
При взгляде на лицевую сторону грани вершины будут располагаться против часовой
стрелки.

Позиция объекта в кадре $i$ имеет шесть степеней свободы и описывается матрицей
\begin{equation}\label{eqn:modelpose}
    \Pose_i = \left[ \begin{array}{ccc|c}
          & \RotMat_i &   & \TrVec_i \\
        \hline
        0 & 0         & 0 & 1
    \end{array} \right] \in \SE
    \,\text{,}
\end{equation}
где матрица $\RotMat_i \in \SO$ задает ориентацию модели, а вектор
$\TrVec_i \in \Real^3$~--- сдвиг.
На вход алгоритма принимается положение объекта на начальном кадре,
а результатом его работы являются последовательно вычисленные позиции
в остальных кадрах.

Проекция $\vect{u} \in \Real^2$ точки на поверхности модели
$\vect{x} \in \Real^3$ описывается стандартной моделью камеры
\begin{equation}\label{eqn:proj}
    \homv{u} = \CamIntr \cdot [\RotMat_i \mid \TrVec_i] \cdot \homv{x}
    \,\text{,}
\end{equation}
где $\homv{u}$ и $\homv{x}$~--- вектора $\vect{u}$ и $\vect{x}$ в однородных
координатах. Функцию, выполняющую проекцию $\vect{x} \mapsto \vect{u}$ в
позиции $\Pose_i$ будем обозначать как $\Projection{\Pose_i}$.

Область изображения, соответствующая проекции объекта (foreground), называется
маской проекции и обозначается $\ImgFg$.
Остальная часть изображения, соответствующая фону (background), обозначается
$\ImgBg = \ImgDom \setminus \ImgFg$.
Границей между $\Omega_f$ и $\Omega_b$ является контур объекта $\Contour$,
задающий
$\CDist \colon \ImgDom \to \Real$ "---
функцию расстояния до контура со знаком
\begin{equation*}
    \CDist(\vect{u}) =
    \begin{cases}
        d(\Contour, \vect{u})  & x \in \ImgFg \\
        -d(\Contour, \vect{u})  & x \in \ImgBg
    \end{cases}
    \,\text{,}
\end{equation*}
где $d(\Contour, \vect{u})$ "--- кратчайшее расстояние от точки $\vect{u}$
до контура $\Contour$.

\subsection{Трекинг на основе точечных особенностей изображения}

Применяемый в данной работе алгоритм трекинга с помощью точечных особенностей
представляет собой разновидность стандартного подхода "--- трекера
Канаде "--- Лукаса "--- Томаси
(KLT-трекера)\cite{LucasAndKanade,TomasiAndKanade,ShiAndTomasi,PyrLK}.
На кадрах с известной позицией объекта выделяются ключевые 2D-точки (точечные
особенности) и определяются соответствующие им 3D-точки на поверхности модели.
Движение ключевых точек отслеживается с помощью вычисления оптического потока.
На кадре, для которого выполняется оценка позиции, по известным 2D-3D
соответствиям вычисляется позиция объекта путем решения задачи
\PnP\cite{LepetitSurvey} с использованием RANSAC\cite{RANSAC} для отсеивания
выбросов.

\Comment{TODO: нужно ли описывать подробнее и рассказывать про улучшения
в виде фильтрации точек?}

\subsection{Метод на основе распределения цвета}

\Comment{TODO: нужно все, что касается чужих методов и их сравнения между
собой перевести в related work без формул, а тут оставить в первую очередь
описание метода, о котором идет речь в статье.}

Метод, использующий распределение цветов основан на построении цветовых
гистограмм, впервые предложенных для трекинга в \cite{Bibby2008}.
По первому изображению, на котором позиция объекта известна, строятся
гистограммы распределения цветов $H_f$ и $H_b$ для маски и для фона
соответственно.
Гистограммы строятся таким образом, чтобы значения в каждом канале разбивались
на 32 ячейки, поэтому в этой главе под цветом пикселя $y$ будем понимать
множество цветов, попадающих в одну ячейку гистограммы: $y \in [0; 31]^3$.
В каждую ячейку гистограммы $H_i$ записывается доля пикселей соответствующих
цветов на области $\Omega_i$ ($i = \{f, b\}$).

В алгоритме PWP3D \cite{PWP3D} используется функция ошибки для позиции, которая
основана на апостериорной вероятности позиции при данном изображении и данном
наборе гистограмм.
Пусть дан новый кадр и некоторая позиция $T$.
Спроецируем объект на изображение с использованием этой позиции.
Получим разбиение изображения на области $\Omega_f$ и $\Omega_b$.
Тогда вероятность того, что $T$
является позицией объекта
$
    P(T) = \prod\limits_{x \in \Omega}(\mathbb{P}(x \in Fg | H_f, H_b)\mathbb{P}(x \in Fg | T) +$ $\mathbb{P}(x \in Bg|H_f, H_b)\mathbb{P}(x \in Bg | T))
$

% TODO: pose enhancement

Здесь $Fg$ - маска проекции, $Bg$ - фон

Тогда
\begin{equation*}
    \mathbb{P}(x \in Fg | T) = 
     \begin{cases}
       1 &x \in \Omega_f\\
       0 &x \in \Omega_b
     \end{cases}
\end{equation*}

Для того, чтобы функция непрерывно зависела от параметров трансформации,
используют приближённую функцию Хевисайда:
$$
    \mathbb{P}(x \in Fg | T) = He(\Phi(x))
$$ 

$$
    \mathbb{P}(x \in Bg | T) = 1 - He(\Phi(x))
$$

Так как гистограммы влияют на принадлежность точки проекции только основываясь
на её цвете,
$\mathbb{P}(x \in Fg | H_f, H_b)$
может оцениваться как
$\mathbb{P}(x \in Fg | I(x) = y)$

В PWP3D

$
    \mathbb{P}(x \in Fg | I(x) = y) = \frac{H_f(y)}{\mathbb{P}(I(x) = y)}
$

$
    \mathbb{P}(x \in Bg | I(x) = y) = \frac{H_b(y)}{\mathbb{P}(I(x) = y)}
$,

где

$
    \mathbb{P}(I(x) = y) = \eta_f H_f(y) + \eta_b H_b(y)
$

$
    \eta_f = \sum\limits_{x \in \Omega}He(\Phi(x))
$

$
    \eta_b = \sum\limits_{x \in \Omega}(1 - He(\Phi(x)))
$

В предлагаемой реализации

$
    \mathbb{P}(x \in Fg | I(x) = y) = \frac{\mathbb{P}(I(x) = y | x \in Fg) \mathbb{P}(x \in Fg)}{\mathbb{P}(I(x) = y)} = \frac{H_f(y)\frac{\eta_f}{\eta_b}}{\mathbb{P}(I(x) = y)}
$

$
    \mathbb{P}(x \in Bg | I(x) = y) = 1 - \mathbb{P}(x \in Fg | I(x) = y)
$.

Далее функция ошибки определяется как
$
    E(\xi) = \sum\limits_{x \in \Omega}\log(He(\Phi(x))\mathbb{P}(x \in Fg | H_f, H_b) + (1 - He(\Phi(x)))\mathbb{P}(x \in Bg|H_f, H_b))
$

После получения новой позиции на каждом следующем кадре строятся новые
гистограммы, которые взвешенно суммируются со старыми: 

$
    H_{f} = \alpha_f H_{f}^{new} + (1 - \alpha_f) H_f^{old}
$

$
    H_{f} = \alpha_b H_{b}^{new} + (1 - \alpha_b) H_b^{old}
$, 

где $\alpha_f = 0.1, \alpha_b = 0.2$ - коэффициенты "забывания".

Чтобы получить оптимальную матрицу трансформации, нужно минимизировать функцию
энергии.
В \cite{Tjaden2018} выведена формула якобиана этой функции, с помощью которого
она минимизируется алгоритмом Гаусса-Ньютона.
В предлагаемой реализации использовался метод последовательного квадратичного
программирования (ссылка), который более устойчив к зашумлённым функциям, хотя
и дольше работает. \Comment{Откуда взято, что SLSQP более устойчив к чему-то,
но дольше работает?}

Минимизация проводится на пирамиде изображений высотой 3 (ссылка?).
\Comment{Вот это вообще не понятно и надо описывать хотя бы минимально}.

\Comment{TODO: описать то, как выбираются гистограммы.}

Одним из недостатков подхода, основанного на распределении цветов, является
неустойчивость к изменению освещения.
Для того, чтобы сделать трекинг устойчивым к таким условиям, изображение
переводится в формат HSV и затем проводится эквилизация гистограмм (ссылка?).
Это ликвидирует изменчивость освещения и может улучшить трекинг в тёмных
сценах, так как в среднем яркость изображения в таких случаях увеличивается.
\Comment{Нужны какие-то внятные объяснения, пока что звучит весьма
сомнительно.}
