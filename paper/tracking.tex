\section{Применяемые методы трекинга}
\subsection{Обозначения}
Под видеопотоком понимается набор из $n$ последовательных изображений $\{I_i\}_{i = 1}^n$, где $I_i: \Omega \rightarrow [0; 255]^3$, $\Omega \subset \mathbb{R}^2$

Позиция объекта описывается матрицей  $T =
\begin{pmatrix}
  R& t\\
  0& 1
\end{pmatrix} \in \mathbb{R}^{4*4}$, в которой $R$ - матрица поворота, а $t$ - вектор переноса. $T$ определяется параметрами Родрига $\xi \in \mathbb{R}^6$ (ссылка?).

Внутренние параметры камеры описываются матрицей $K = 
\begin{pmatrix}
  f_x& 0& c_x\\
  0& f_y& c_y\\
  0& 0& 1
\end{pmatrix}$

Обозначим за $X \in \mathbb{R}^4$ точку на модели в однородных координатах. Тогда её проекция на плоскость вычисляется как $x = \pi(K(R | t) X)$, где $\pi(
(\tilde{X_1}\ \tilde{X_2}\ \tilde{X_3})^T
) = (\frac{\tilde{X_1}}{\tilde{X_3}} \ \frac{\tilde{X_2}}{\tilde{X_3}})^T$.

$x \in \mathbb{R}^2$ является точкой на плоскости изображения. Если $x \in \Omega$, то $I(x) = y \in [0; 255]^3$ - её цвет в формате RGB.

Область изобажения, которая соответствует проекции объекта (foreground), называется маской проекции и обозначается $\Omega_f$. Остальная часть изображения, соответствующая фону (background), обозначается $\Omega_b = \Omega \setminus \Omega_f$. Границей между $\Omega_f$ и $\Omega_b$ является контур объекта $c$. Он определяет функцию расстояния до контура со знаком: 
\begin{equation*}
\Phi(x) = 
 \begin{cases}
   d(x, c) &x \in \Omega_f\\
   -d(x, c) &x \in \Omega_b
 \end{cases}
\end{equation*}

На вход алгоритму подаётся набор изображений $\{I_i\}_{i = 1}^n$, модель объекта в виде полигональной сетки и позиция объекта на первом кадре $T_1$. Требуется найти позиции $T_2, ..., T_n$ на последующих кадрах.

\subsection{Метод на основе ключевых точек}
Алгоритм трекинга на ключевых точках основан на алгоритме Лукаса-Канаде~\cite{LukasKanade} (ещё ссылки?). На первом кадре детектируются ключевые точки, и выбираются те из них, которые принадлежат проекции модели. Так как на первом кадре позиция объекта считается известной, для каждой точки может быть найден её прообраз на модели. Затем для каждого следующего кадра вычисляется оптический поток, и определяется движение сдетектированных на предыдущем кадре точек. Таким образом, мы получаем на новом изображении позиции точек, для которых известны их прообразы на модели. Решая задачу Perspective-n-Point (ссылку сюда) алгоритмом ePnP~\cite{Lepetit}, находим движение камеры между кадрами или движение объекта, если считать, что камера неподвижна. После нахождения позиции объекта фильтруются выбросы -- точки, которые при проецировании объекта из заданной позиции отстоят от соответствующих ключевых особенностей больше, чем на определённое значение. 

\subsection{Метод на основе распределения цвета}
Метод, использующий распределение цветов основан на построении цветовых гистограмм, впервые предложенных для трекинга в \cite{Bibby2008}. По первому изображению, на котором позиция объекта известна, строятся гистограммы распределения цветов $H_f$ и $H_b$ для маски и для фона соответственно. Гистограммы строятся таким образом, чтобы значения в каждом канале разбивались на 32 ячейки, поэтому в этой главе под цветом пикселя $y$ будем понимать множество цветов, попадающих в одну ячейку гистограммы: $y \in [0; 31]^3$. В каждую ячейку гистограммы $H_i$ записывается доля пикселей соответствующих цветов на области $\Omega_i$ ($i = \{f, b\}$). 

В алгоритме PWP3D \cite{PWP3D} используется функция ошибки для позиции, которая основана на апостериорной вероятности позиции при данном изображении и данном наборе гистограмм. Пусть дан новый кадр и некоторая позиция $T$. Спроецируем объект на изображение с использованием этой позиции. Получим разбиение изображения на области $\Omega_f$ и $\Omega_b$. Тогда вероятность того, что $T$ является позицией объекта $P(T) = \prod\limits_{x \in \Omega}(\mathbb{P}(x \in Fg | H_f, H_b)\mathbb{P}(x \in Fg | T) +$ $\mathbb{P}(x \in Bg|H_f, H_b)\mathbb{P}(x \in Bg | T))$

% TODO: pose enhancement

Здесь $Fg$ - маска проекции, $Bg$ - фон

Тогда
\begin{equation*}
\mathbb{P}(x \in Fg | T) = 
 \begin{cases}
   1 &x \in \Omega_f\\
   0 &x \in \Omega_b
 \end{cases}
\end{equation*}

Для того, чтобы функция непрерывно зависела от параметров трансформации, используют приближённую функцию Хевисайда:

$$\mathbb{P}(x \in Fg | T) = He(\Phi(x))$$ 

$$\mathbb{P}(x \in Bg | T) = 1 - He(\Phi(x))$$

Так как гистограммы влияют на принадлежность точки проекции только основываясь на её цвете, $\mathbb{P}(x \in Fg | H_f, H_b)$ может оцениваться как $\mathbb{P}(x \in Fg | I(x) = y)$

В PWP3D 

$\mathbb{P}(x \in Fg | I(x) = y) = \frac{H_f(y)}{\mathbb{P}(I(x) = y)}$

$\mathbb{P}(x \in Bg | I(x) = y) = \frac{H_b(y)}{\mathbb{P}(I(x) = y)}$, 

где $\mathbb{P}(I(x) = y) = \eta_f H_f(y) + \eta_b H_b(y)$,

$\eta_f = \sum\limits_{x \in \Omega}He(\Phi(x))$

$\eta_b = \sum\limits_{x \in \Omega}(1 - He(\Phi(x)))$

В предлагаемой реализации $\mathbb{P}(x \in Fg | I(x) = y) = \frac{\mathbb{P}(I(x) = y | x \in Fg) \mathbb{P}(x \in Fg)}{\mathbb{P}(I(x) = y)} = \frac{H_f(y)\frac{\eta_f}{\eta_b}}{\mathbb{P}(I(x) = y)}$

$\mathbb{P}(x \in Bg | I(x) = y) = 1 - \mathbb{P}(x \in Fg | I(x) = y)$

Далее функция ошибки определяется как $E(\xi) = \sum\limits_{x \in \Omega}\log(He(\Phi(x))\mathbb{P}(x \in Fg | H_f, H_b) + (1 - He(\Phi(x)))\mathbb{P}(x \in Bg|H_f, H_b))$

После получения новой позиции на каждом следующем кадре строятся новые гистограммы, которые взвешенно суммируются со старыми: 

$H_{f} = \alpha_f H_{f}^{new} + (1 - \alpha_f) H_f^{old}$

$H_{f} = \alpha_b H_{b}^{new} + (1 - \alpha_b) H_b^{old}$, 

где $\alpha_f = 0.1, \alpha_b = 0.2$ - коэффициенты "забывания".

Чтобы получить оптимальную матрицу трансформации, нужно минимизировать функцию энергии. В \cite{Tjaden2018} выведена формула якобиана этой функции, с помощью которого она минимизируется алгоритмом Гаусса-Ньютона. В предлагаемой реализации использовался метод последовательного квадратичного программирования (ссылка), который более устойчив к зашумлённым функциям, хотя и дольше работает. Минимизация проводится на пирамиде изображений высотой 3 (ссылка?).

В \cite{Tjaden2017} предложено использовать не одну пару гистограмм для модели, а по две гистограммы для каждой вершины. В таком случае область действия каждой гистограммы сокращается до круга определённого радиуса вокруг проекции соответствующей вершины. Это позволяет учитывать информацию о цветах локально. Такое количество данных может быть избыточно. В \cite{RegionPhotometric} предлагается способ ограничить это количество, но он основан на разбиении пространства изображения на секторы, и  в случае поворота объекта отдельная гистограмма фиксирует цвета с разных частей объекта, тогда как в \cite{Tjaden2017} гистограммы привязаны к определённым вершинам и хранят информацию только об окрестностях их проекций. В предлагаемом алгоритме выбран именно этот подход, но гистограммы строятся не для всех вершин, а только для 10\% из них.

Одним из недостатков подхода, основанного на распределении цветов, является неустойчивость к изменению освещения. Для того, чтобы сделать трекинг устойчивым к таким условиям, изображение переводится в формат HSV и затем проводится эквилизация гистограмм (ссылка?). Это ликвидирует изменчивость освещения и может улучшить трекинг в тёмных сценах, так как в среднем яркость изображения в таких случаях увеличивается.